{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d8792-24d5-4061-be6f-1cb0870ec4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "mport Libraries and Define Auxiliary Functions\n",
    "We will import the following libraries for the lab\n",
    "In [33]:\n",
    "# Pandas is a software library written for the Python programming language for data manipulation and analysis.\n",
    "import pandas as pd\n",
    "# NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\n",
    "import numpy as np\n",
    "# Matplotlib is a plotting library for python and pyplot gives us a MatLab like plotting framework. We will use this in our plotter function to plot data.\n",
    "import matplotlib.pyplot as plt\n",
    "#Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics\n",
    "import seaborn as sns\n",
    "# Preprocessing allows us to standarsize our data\n",
    "from sklearn import preprocessing\n",
    "# Allows us to split our data into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Allows us to test parameters of classification algorithms and find the best one\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Logistic Regression classification algorithm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Support Vector Machine classification algorithm\n",
    "from sklearn.svm import SVC\n",
    "# Decision Tree classification algorithm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# K Nearest Neighbors classification algorithm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "This function is to plot the confusion matrix.\n",
    "In [34]:\n",
    "def plot_confusion_matrix(y,y_predict):\n",
    "    \"this function plots the confusion matrix\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm = confusion_matrix(y, y_predict)\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(['did not land', 'land']); ax.yaxis.set_ticklabels(['did not land', 'landed'])\n",
    "Load the dataframe\n",
    "Load the data\n",
    "In [35]:\n",
    "data = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/dataset_part_2.csv\")\n",
    "\n",
    "# If you were unable to complete the previous lab correctly you can uncomment and load this csv\n",
    "\n",
    "# data = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0701EN-SkillsNetwork/api/dataset_part_2.csv')\n",
    "\n",
    "data.head()\n",
    "\n",
    "data_graph = {}\n",
    "In [36]:\n",
    "X = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/dataset_part_3.csv')\n",
    "\n",
    "# If you were unable to complete the previous lab correctly you can uncomment and load this csv\n",
    "\n",
    "# X = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0701EN-SkillsNetwork/api/dataset_part_3.csv')\n",
    "\n",
    "X.head(100)\n",
    "Out[36]:\n",
    "FlightNumber\tPayloadMass\tFlights\tBlock\tReusedCount\tOrbit_ES-L1\tOrbit_GEO\tOrbit_GTO\tOrbit_HEO\tOrbit_ISS\t...\tSerial_B1058\tSerial_B1059\tSerial_B1060\tSerial_B1062\tGridFins_False\tGridFins_True\tReused_False\tReused_True\tLegs_False\tLegs_True\n",
    "0\t1.0\t6104.959412\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
    "1\t2.0\t525.000000\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
    "2\t3.0\t677.000000\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
    "3\t4.0\t500.000000\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
    "4\t5.0\t3170.000000\t1.0\t1.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "85\t86.0\t15400.000000\t2.0\t5.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t1.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\n",
    "86\t87.0\t15400.000000\t3.0\t5.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t1.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\n",
    "87\t88.0\t15400.000000\t6.0\t5.0\t5.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\n",
    "88\t89.0\t15400.000000\t3.0\t5.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t1.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\n",
    "89\t90.0\t3681.000000\t1.0\t5.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t1.0\t0.0\t0.0\t1.0\n",
    "90 rows Ã— 83 columns\n",
    "TASK 1\n",
    "Create a NumPy array from the column Class in data, by applying the method to_numpy() then assign it to the variable Y,make sure the output is a Pandas series (only one bracket df['name of column']).\n",
    "In [37]:\n",
    "Y = data['Class'].to_numpy()\n",
    "TASK 2\n",
    "Standardize the data in X then reassign it to the variable  X using the transform provided below.\n",
    "In [38]:\n",
    "# students get this \n",
    "transform = preprocessing.StandardScaler()\n",
    "In [39]:\n",
    "X = transform.fit_transform(X)\n",
    "We split the data into training and testing data using the function  train_test_split. The training data is divided into validation data, a second set used for training data; then the models are trained and hyperparameters are selected using the function GridSearchCV.\n",
    "TASK 3\n",
    "Use the function train_test_split to split the data X and Y into training and test data. Set the parameter test_size to 0.2 and random_state to 2. The training data and test data should be assigned to the following labels.\n",
    "X_train, X_test, Y_train, Y_test\n",
    "In [40]:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size=0.2, random_state=2)\n",
    "print ('Train set:', X_train.shape,  Y_train.shape)\n",
    "print ('Test set:', X_test.shape,  Y_test.shape)\n",
    "Train set: (72, 83) (72,)\n",
    "Test set: (18, 83) (18,)\n",
    "we can see we only have 18 test samples.\n",
    "In [41]:\n",
    "Y_test.shape\n",
    "Out[41]:\n",
    "(18,)\n",
    "TASK 4\n",
    "Create a logistic regression object then create a GridSearchCV object  logreg_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.\n",
    "In [42]:\n",
    "parameters ={'C':[0.01,0.1,1],\n",
    "             'penalty':['l2'],\n",
    "             'solver':['lbfgs']}\n",
    "In [43]:\n",
    "lr = LogisticRegression()\n",
    "grid_search = GridSearchCV(lr, parameters, cv=10)\n",
    "logreg_cv = grid_search.fit(X_train, Y_train)\n",
    "We output the GridSearchCV object for logistic regression. We display the best parameters using the data attribute best_params\\_ and the accuracy on the validation data using the data attribute best_score\\_.\n",
    "In [44]:\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)\n",
    "data_graph['Logistic Regression'] = logreg_cv.best_score_\n",
    "tuned hpyerparameters :(best parameters)  {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
    "accuracy : 0.8464285714285713\n",
    "TASK 5\n",
    "Calculate the accuracy on the test data using the method score:\n",
    "In [45]:\n",
    "logreg_cv.score(X_test, Y_test)\n",
    "Out[45]:\n",
    "0.8333333333333334\n",
    "Lets look at the confusion matrix:\n",
    "In [46]:\n",
    "yhat=logreg_cv.predict(X_test)\n",
    "plot_confusion_matrix(Y_test,yhat)\n",
    "\n",
    "Examining the confusion matrix, we see that logistic regression can distinguish between the different classes. We see that the major problem is false positives.\n",
    "TASK 6\n",
    "Create a support vector machine object then create a  GridSearchCV object  svm_cv with cv - 10. Fit the object to find the best parameters from the dictionary parameters.\n",
    "In [47]:\n",
    "svm_parameters = {'kernel':('linear', 'rbf','poly','rbf', 'sigmoid'),\n",
    "              'C': np.logspace(-3, 3, 5),\n",
    "              'gamma':np.logspace(-3, 3, 5)}\n",
    "svm = SVC()\n",
    "In [48]:\n",
    "grid_search = GridSearchCV(svm, svm_parameters, cv=10)\n",
    "svm_cv = grid_search.fit(X_train, Y_train)\n",
    "In [49]:\n",
    "print(\"tuned hpyerparameters :(best parameters) \",svm_cv.best_params_)\n",
    "print(\"accuracy :\",svm_cv.best_score_)\n",
    "data_graph['SVM'] = svm_cv.best_score_\n",
    "tuned hpyerparameters :(best parameters)  {'C': 1.0, 'gamma': 0.03162277660168379, 'kernel': 'sigmoid'}\n",
    "accuracy : 0.8482142857142856\n",
    "TASK 7\n",
    "Calculate the accuracy on the test data using the method score:\n",
    "In [50]:\n",
    "svm_cv.score(X_test, Y_test)\n",
    "Out[50]:\n",
    "0.8333333333333334\n",
    "We can plot the confusion matrix\n",
    "In [51]:\n",
    "yhat=svm_cv.predict(X_test)\n",
    "plot_confusion_matrix(Y_test,yhat)\n",
    "\n",
    "TASK 8\n",
    "Create a decision tree classifier object then create a  GridSearchCV object  tree_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.\n",
    "In [52]:\n",
    "parameters = {'criterion': ['gini', 'entropy'],\n",
    "     'splitter': ['best', 'random'],\n",
    "     'max_depth': [2*n for n in range(1,10)],\n",
    "     'max_features': ['auto', 'sqrt'],\n",
    "     'min_samples_leaf': [1, 2, 4],\n",
    "     'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "In [53]:\n",
    "grid_search = GridSearchCV(tree, parameters, cv=10)\n",
    "tree_cv = grid_search.fit(X_train, Y_train)\n",
    "In [54]:\n",
    "print(\"tuned hpyerparameters :(best parameters) \",tree_cv.best_params_)\n",
    "print(\"accuracy :\",tree_cv.best_score_)\n",
    "data_graph['Decision Tree'] = tree_cv.best_score_\n",
    "tuned hpyerparameters :(best parameters)  {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'random'}\n",
    "accuracy : 0.8892857142857145\n",
    "TASK 9\n",
    "Calculate the accuracy of tree_cv on the test data using the method score:\n",
    "In [55]:\n",
    "tree_cv.score(X_test, Y_test)\n",
    "Out[55]:\n",
    "0.8333333333333334\n",
    "We can plot the confusion matrix\n",
    "In [56]:\n",
    "yhat = tree_cv.predict(X_test)\n",
    "plot_confusion_matrix(Y_test,yhat)\n",
    "\n",
    "TASK 10\n",
    "Create a k nearest neighbors object then create a  GridSearchCV object  knn_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.\n",
    "In [57]:\n",
    "parameters = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "              'p': [1,2]}\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "In [58]:\n",
    "grid_search = GridSearchCV(KNN, parameters, cv=10)\n",
    "knn_cv = grid_search.fit(X_train, Y_train)\n",
    "In [59]:\n",
    "print(\"tuned hpyerparameters :(best parameters) \",knn_cv.best_params_)\n",
    "print(\"accuracy :\",knn_cv.best_score_)\n",
    "data_graph['KNN'] = knn_cv.best_score_\n",
    "tuned hpyerparameters :(best parameters)  {'algorithm': 'auto', 'n_neighbors': 10, 'p': 1}\n",
    "accuracy : 0.8482142857142858\n",
    "TASK 11\n",
    "Calculate the accuracy of tree_cv on the test data using the method score:\n",
    "In [60]:\n",
    "knn_cv.score(X_test, Y_test)\n",
    "Out[60]:\n",
    "0.8333333333333334\n",
    "We can plot the confusion matrix\n",
    "In [61]:\n",
    "yhat = knn_cv.predict(X_test)\n",
    "plot_confusion_matrix(Y_test,yhat)\n",
    "\n",
    "In [63]:\n",
    "# creating the dataset\n",
    "\n",
    "courses = list(data_graph.keys())\n",
    "values = list(data_graph.values())\n",
    "  \n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(courses, values, color ='teal',\n",
    "        width = 0.4)\n",
    " \n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"MODELS SCORE \")\n",
    "plt.show()\n",
    "\n",
    "TASK 12\n",
    "Find the method performs best:\n",
    "All the model almost performed similarly.\n",
    "Authors\n",
    "Joseph Santarcangelo has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n",
    "Change Log\n",
    "Date (YYYY-MM-DD)\tVersion\tChanged By\tChange Description\n",
    "2021-08-31\t1.1\tLakshmi Holla\tModified markdown\n",
    "2020-09-20\t1.0\tJoseph\tModified Multiple Areas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
